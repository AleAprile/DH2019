Konferenzbericht DH2019 Utrecht von Monika Barget, IEG Mainz

Die internationale DH2019 Konferenz stand unter dem Motto „complexities“ und brachte Forschende aus den digitalen Geisteswissenschaften und technischen Disziplinen in der Utrechter Konzerthalle Tivoli Vredenburg zusammen. Das Ziel, mehr Teilnehmer*innen aus Südamerika, Südeuropa, Osteuropa, Afrika und Südostasien an der Konferenz zu beteiligen und ihre Forschungsinteressen stärker zu berücksichtigen (vgl. http://www.globaloutlookdh.org/around-dh-2020-and-quantifying-dh-infographic-revisited/), konnte nicht erreicht werden, weil sowohl die Kosten für Unterkunft und Verpflegung in Utrecht als auch die hohe Konferenzgebühr eine große Hürde darstellten. Somit blieben die erste Keynote von Prof. Francis Nyamnjoh aus Südafrika und eine gesonderte Präsentation afrikanischer Poster (https://dh2019.adho.org/programme/focus-on-africa/) die einzigen Programmpunkte, deren Protagonist*innen nicht aus den führenden westlichen Industrienationen, den finanzstarken Vereinigten Arabischen Emiraten oder den High-Tech-Nationen Asiens kamen. 
Auch stilistisch und inhaltlich hob sich Prof. Francis B. Nyamnjohs (http://www.anthropology.uct.ac.za/san/people/academic/nyamnjoh) Vorlesung von den übrigen Beiträgen ab. Humorvoll, anekdotisch und mit zahlreichen Bezügen zu afrikanischer Spiritualität verglich Francis Nyamnjoh die von Beginn an interdisziplinär ausgerichteten digitalen Geisteswissenschaften mit Juju, „the African belief in the compositeness of being human“. Er sprach über Unvollkommenheit als Ressource und Chance der Verknüpfung, womit er nicht nur zu den vor-technischen Grundlagen der geisteswissenschaftlichen Disziplinen zurückführte, sondern auch die supra-technischen Verpflichtungen der gegenwärtigen digitalisierten Forschung aufzeigte.
Die meisten Vorträge in den Panels widmeten sich konkreten Projekten oder detaillierten technischen Problemen aus allen Bereichen der Digital Humanities von Netzwerkanalyse über Topic Modelling bis hin zur Annotation audiovisueller Daten. Die besonderen Anforderungen mehrsprachiger oder anderweitig hybrider Quellen standen dabei – passend zum Konferenzthema – im Vordergrund.	

Für die derzeitigen Forschungen am Institut für Europäische Geschichte in Mainz besonders relevant sind die folgenden Themen:	
	
1) Aufbau nationaler Forschungsinfrastrukturen	

Nicolas Larrousse präsentierte die französische Infrastruktur (https://www.huma-num.fr/) für die Geisteswissenschaften, die als Teil der europäischen ESFRI Roadmap über ebenso viele Fördergelder verfügt wie große naturwissenschaftliche Projekte. Trotz der hervorragenden finanziellen Ausstattung bleibt es jedoch eine kontinuierliche Herausforderung, den „mesh of servers and mix of technologies“ zu koordinieren, in dem vor allem der Faktor Mensch berücksichtigt werden muss. Ohne ein strukturiertes Netz von wissenschaftlichen Mitarbeiter*innen, lokalen Systemadministrator*innen, Account Manager*innen und Multiplikator*innen sei eine nationale Forschungsinfrastruktur nicht möglich. Das gesamte Personal und die verschiedenen Anwendergruppen müssen regelmäßig geschult werden. Die französische Infrastruktur Huma-Num leistet dies durch Workshops, sog. „huma-num bars“ und „MSH Tours“. Letztere Events werden von der Maison des Sciences de l’Homme organisiert und bieten eine nutzerorientierte Einführung in die Dienstleistungen und digitalen Werkzeuge, die Huma-Num zur Verfügung stellt. Das größte Hindernis im Vernetzen von Daten sei dabei oft der politische Wille, nicht die technische Machbarkeit.	

2) Geohumanities
Mehrere Vorträge befassten sich mit dem sog. Deep Mapping, das interaktive Karten um Bilder, Audiodateien, Videos, Links und Annotationen ergänzt (z.B. „Semantic Deep Mapping in an Integrated Platform for Studying Historical Amsterdam“ von Julia Noordegraaf und Team). Deep Mapping bietet sich besonders an, wenn der geographische Informationswert vorhandener Daten beschränkt ist, oder wenn Räumlichkeit auf verschiedenen Wahrnehmungsebenen dargestellt werden soll. Daher wird Deep Mapping in geschichtswissenschaftlichen, archäologischen und kunsthistorischen Projekten angewandt, die komplexe Rekonstruktionen epochenspezifischer Räume anstreben. Auch 3-D-Animationen können integriert werden. 
Im Bereich der historischen Kartographie wachsen zudem die „Gazetteers“ (Verzeichnisse, die Ortsnamen bestimmten geographischen Daten und Zeitfenstern zuordnen sowie deren Varianten erfassen), von denen viele einen regionalen Fokus haben. 	
Vincent Jolivet und Julien Pilla stellten in der Postersession ihr Projekt „Le Dictionnaire topographique. Une API pour les toponymes anciens français“ (http://cths.fr/dico-topo/) vor, das auf einer französischen Sammlung historischer Ortsbezeichnungen aus dem 19. Jahrhundert beruht. Die selektive Erfassung der Ortsnamen in der gewählten Quelle führt allerdings zu einem geographischen Ungleichgewicht der Datenverteilung innerhalb Frankreichs. Für einige Regionen liegen gar keine Daten vor. Zudem wurden politische oder kulturelle Funktionen der Orte nur rudimentär erfasst, weshalb es nicht möglich ist, gezielt nach allen Kirchorten oder Verwaltungssitzen zu suchen. Ein Gazetteer-Projekt, das sich der globalen Sammlung historischer Ortnamen widmet, ist demgegenüber der World Historical Gazetteer (http://whgazetteer.org/). Karl Grosser und Ruth M. Mostern (beide University of Pittsburgh) stellten den aktuellen Stand des Projekts vor und baten um Mitarbeit bei der Datenerhebung und bei der Entwicklung der Ontologie.
Inwiefern Georeferenzierung nach modernen Standards und das Rektifizieren historischer Karten unser Verständnis der Vergangenheit vertieft, wurde nach dem Vortrag von Martin Reckziegel und David Joseph Wrisley diskutiert. Die beiden Wissenschaftler versuchen, mittelalterliche OT-Karten/Mappae Mundi zu rektifizieren und aus den verschiedenen Graden der Verzerrung Rückschlusse auf die Funktion dieser Karten zu ziehen. Diese Herangehensweise stieß allerdings bei Historikern im Publikum auf Kritik, die eine Beschränkung auf die technischen Möglichkeiten klassischer Geoinformationssysteme (GIS) ablehnen. In der Tat sind viele historische Geodaten nicht lokalisierbar, weshalb viele geisteswissenschaftliche Anwendungen heute auch Erfahrungsdistanzen zwischen fiktiven oder unbekannten Orten visualisieren und Räume als ständig veränderbare Produkte von Bewegung erfassen. 

3) Texterkennung (OCR)	

Eine deutliche Verbesserung der automatisierten Texterkennung gelang einem Team der Universität Zürich in der Arbeit an Ausgaben der Neuen Züricher Zeitung aus dem 19. Jahrhundert. Wie Phillip Benjamin Ströbel und Simon Clematide berichteten, ließ die Scanqualität der Originale zu wünschen übrig, da die Seiten oft fleckig oder durchscheinend waren. Mit Transkribus (https://transkribus.eu/Transkribus/) und sorgfältig ausgewählten Trainingsdaten war es schließlich möglich, für die digitalisierten Ausgaben der NZZ eine Genauigkeit von 95% zu erzielen. Diese Resultate waren der ebenfalls getesteten Leistung des Abbyy Fine Reader (https://www.abbyy.com/en-us/finereader/) überlegen, und auch für Tests mit anderen historischen Zeitungen aus der Schweiz ergab Transkribus bessere OCR-Ergebnisse. Vergleiche mit Tesseract (https://github.com/tesseract-ocr/) oder Kraken (http://kraken.re/) waren bislang nicht möglich. Den aktuellen Stand des Kraken-Projektes stellte im selben Panel Benjamin Kiessling (Université PSL, Universität Leipzig) vor. Der große Vorteil von Kraken besteht darin in, dass es problemlos in andere Software integriert werden kann. Und genau wie Transkribus kann es für verschiedene Text- und Schriftarten optimiert werden. Die Nutzung von Kraken wird seitens der Entwickler nicht nachverfolgt, und es besteht keine Pflicht, eigene Daten mit dem Projekt und der Community zu teilen. 	


4) Interfaces/Webseiten	

 Die Interaktion zwischen Mensch und Computer sowie die Einbeziehung von Nutzererfahrungen in das Design von Plattformen wurden in den täglichen „Tools, Interfaces and Infrastructures“ Panels diskutiert. Hier ging es u.a. um die Gestaltung von Webseiten, die Verbesserung der Webseitennavigation, und um die Analyse von Nutzergewohnheiten im Umgang mit Suchmaschinen. Gerben Zaagsma befasste sich mit den „Politics of Digitization“ und evaluierte die Internetauftritte jüdischer digitaler Archive, britische Forscher präsentierten ihre Auswertung von Such- und Zugriffverhalten auf elektronische Fachbücher und Zeitschriften innerhalb des Vereinigten Königreichs. Monika Barget und Susan Schreibman erklärten die besonderen Anforderungen an kollaborative „Public Humanities“-Frontends am Beispiel des Letters 1916-1923 Projekts (www.letters1916.ie).	

 

5) Digitale Quellenkritik und „Digital Literacy“	

In der Postersession wurde die interaktive Luxemburger Lehr- und Lernplattform für digitale Quellenkritik Ranke2 (https://ranke2.uni.lu/) präsentiert, zu der auch auswärtige Wissenschaftler*innen beitragen können. Da „digital literacy“ und ein kritischer Umgang mit digitalen oder digitalisierten Quellen in vielen europäischen Ländern die Bildungspolitik bewegen, sind die sieben kurzen Einführungen in die Geschichte des Internets und den „digital turn“ in den Geisteswissenschaften, die Ranke2 bislang zur Verfügung stellt, eine wichtige Anregung, um weitere Trainingsmaterialien für verschiedene Zielgruppen zu entwickeln.

6) Datenvisualisierung und Datenambiguität
Die Abbildung von “uncertainty” stellt nach wie vor eine große Herausforderung für alle Formen der Datenvisualisierung dar. Viele Projekte entscheiden sich für ein Ampel-System mit drei Zuverlässigkeitsstufen. Eveline Wandl-Vogt, Enric Senabre Hidalgo und Roberto Theron plädierten in ihrem Workshop für einen „Co-Design Approach” in der geisteswissenschaftlichen Datendarstellung. In ihrem PROVIDEDH Projekt (“PROgressive VIsual DEcision-Making in Digital Humanities”, https://twitter.com/hashtag/provideh) entwerfen sie interaktive visuelle Werkzeuge, die den Unsicherheitsgrad von Ausgangsdaten und Rechenmodellen abbilden, sowie bei der schrittweisen Integration verbesserter Datensätze helfen. Auch im Workshop von Cesar Gonzalez-Perez und Patricia Martín-Rodilla stand die Abbildung von Datenambiguität im Mittelpunkt. Die konzeptionelle Modellierungssprache ConML  (http://www.conml.org/) soll sowohl zur transparenten Erfassung möglicher Unzuverlässigkeit der Daten als auch zur Darstellung relativer Gültigkeit von Daten in Raum und Zeit beitragen. 


7) Neue Tools und Bibliotheken für das „Distant Reading“	

Max Grüntgens und Thomas Kollatz (beide Akademie der Wissenschaften, Mainz) boten den Workshop „XML2RDF – extracting RDF statements from XML resources with XTriples“ an, der Methoden der Datenserialisierung auf Basis des Resource Description Framework (RDF) erklärte. Ein internationales Forscherteam organisierte den Workshop „Introduction to Natural Language Processing for DH Research with SpaCy – A Fast and Accessible Library that Integrates Modern Machine Learning Technology”. Dieser Kurs richtete sich auch an weniger erfahrene Anwender*innen. Immer mehr DH-Tools im Bereich „distant reading“ bzw. „natural language processing“ versprechen erfolgreiches Arbeiten mit weniger Vorkenntnissen und weniger Aufwand, aber es scheint bislang keine objektive Qualitätskontrolle oder Entscheidungshilfen für Anwender*innen zu geben. Da selten dieselben Corpora oder dieselben Fragestellungen mit mehreren Tools analysiert werden, fehlt eine vergleichende Kritik der Ergebnisse und ihrer Visualisierung.  	

 

8) DH als wissenschaftliche Disziplin	

Wie auf allen größeren DH Konferenzen bislang setzte sich auch in Utrecht die disziplinäre Standortbestimmung der digitalen Geisteswissenschaften fort. Ein Vortrag problematisierte das Verhältnis von Data Science und Digital Humanities. Diese Debatte griff die zweite Keynote von Prof. em. Tito Orlandi aus Rom auf. Er erinnerte an die daten-orientierten, modell-getriebenen Ursprünge der DH als CH (Humanities Computing, ital. l’informatica umanistica) und wehrte sich dagegen, sie zu einer Randdisziplin innerhalb der digitalen Geisteswissenschaften zu degradieren. 	
Er stellte die bisherige Geschichte des starken technisch-mathematischen Einflusses in den digitalen Geisteswissenschaften anhand der wichtigsten Publikationen seit 1946 dar. (Vgl. auch Susan Hockey, The History of Humanities Computing, in: A Companion to Digital Humanities, ed. Susan Schreibman, Ray Siemens, John Unsworth. Oxford: Blackwell, 2004: http://www.digitalhumanities.org/companion/view?docId=blackwell/9781405103213/9781405103213.xml&chunk.id=ss1-2-1) Prof. Orlandi rief die Geisteswissenschaftler*innen in der DH Community dazu auf, die Kenntnis von Computern und Programmiersprachen nicht allein den Technikern in ihren Team zu überlassen, da Grundkenntnisse wichtig für die richtige Anwendung der technischen Möglichkeiten seien. In seinem Abstract schrieb Prof. Orlandi:

„From this point of view, one may affirm that the Digital Humanities has won its struggle to obtain official recognition. On the other hand, despite this achievement the impression remains that many of the methodological discussions of the first decades have not been sufficiently digested, and have not generated some kind of general agreement — a kind of “Kuhnian paradigm” of the discipline. The main scope of my lecture is to get back to the old discussions, with the benefit of new hindsights, to see what we can learn from them to address today’s problems, if properly evaluated.”
Die abschließende Keynote von Prof. Johanna Drucker (https://dma.ucla.edu/faculty/profiles/?ID=99) schloss den Kreis zum philosophischen Einstieg von Prof. Francis Nyamnjoh: sie definierte Nachhaltigkeit in den digitalen Geisteswissenschaften nicht nur als ständige Aktualisierung von Software, sondern als ethische Verantwortung in der Produktion, Verwendung und Entsorgung von Hardware, deren Rohstoff oft unter unmenschlichen Bedingungen abgebaut und gehandelt werden – zum Nachteil jenes globalen Südens, der auf der wichtigsten internationalen DH Konferenz weitgehend unsichtbar geblieben ist.	


Alle Beiträge der DH2019 können im „Book of Abstracts“ nachgeschlagen werden: https://dh2019.adho.org/programme/book-of-abstracts/
Auf Twitter finden sich Fotos, Meinungen und Kommentare unter #DH2019.  

